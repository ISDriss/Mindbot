{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from scipy.signal import butter, lfilter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bandpass_filter(data, lowcut=1.0, highcut=40.0, fs=256, order=5):\n",
    "    \"\"\"Applique un filtre passe-bande pour ne garder que les fr√©quences utiles (1-40 Hz).\"\"\"\n",
    "    nyquist = 0.5 * fs\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return lfilter(b, a, data, axis=0)\n",
    "\n",
    "def normalize_eeg(data):\n",
    "    \"\"\"Normalise les EEG en appliquant un Z-score pour stabiliser les valeurs.\"\"\"\n",
    "    return (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n",
    "\n",
    "def preprocess_eeg(data, fs=256):\n",
    "    \"\"\"Pipeline complet de pr√©traitement des signaux EEG.\"\"\"\n",
    "    data = bandpass_filter(data, lowcut=1.0, highcut=40.0, fs=fs)  # Filtrage\n",
    "    data = normalize_eeg(data)  # Normalisation\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EEG Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, data_dir, fs=256):\n",
    "        \"\"\"Dataset PyTorch pour charger et pr√©traiter les EEG.\n",
    "        \n",
    "        Args:\n",
    "            data_dir (str): Chemin vers le dossier contenant les fichiers CSV.\n",
    "            fs (int, optional): Fr√©quence d'√©chantillonnage des EEG. Default: 256 Hz.\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.files = os.listdir(data_dir)\n",
    "        self.fs = fs  # Fr√©quence d'√©chantillonnage\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Charge un fichier EEG, applique le pr√©traitement et retourne les donn√©es avec le label.\"\"\"\n",
    "        file_name = self.files[idx]\n",
    "        file_path = os.path.join(self.data_dir, file_name)\n",
    "\n",
    "        # Chargement du fichier CSV\n",
    "        df = pd.read_csv(file_path)\n",
    "        eeg_data = df.iloc[:, :-1].values  # Exclure la derni√®re colonne (timestamp ou label)\n",
    "        \n",
    "        # Appliquer le pr√©traitement\n",
    "        eeg_data = preprocess_eeg(eeg_data, fs=self.fs)\n",
    "        \n",
    "        # Convertir en tenseur PyTorch et ajouter une dimension pour la convolution\n",
    "        eeg_data = torch.tensor(eeg_data, dtype=torch.float32).unsqueeze(0)  # [1, channels, time]\n",
    "\n",
    "        # Extraire le label √† partir du nom du fichier\n",
    "        label_name, _ = file_name.split(\"_\")\n",
    "        label_map = {\"stop\": 0, \"avant\": 1, \"arri√®re\": 2, \"gauche\": 3, \"droite\": 4}\n",
    "        label = label_map[label_name]\n",
    "        label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return eeg_data, label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.getcwd(), \"data\")  # data directory\n",
    "dataset = EEGDataset(data_dir)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define EEGNet model and Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGNet(nn.Module):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super(EEGNet, self).__init__()\n",
    "        \n",
    "        # Premi√®re Convolution : Extraction initiale des caract√©ristiques EEG\n",
    "        self.conv1 = nn.Conv2d(1, 16, (1, 64), padding=(0, 32))  \n",
    "        self.batchnorm1 = nn.BatchNorm2d(16)  # Normalisation des features\n",
    "        \n",
    "        # Convolution Spatiale : Applique un filtre sur plusieurs √©lectrodes\n",
    "        self.conv2 = nn.Conv2d(16, 32, (2, 1))  \n",
    "        \n",
    "        # Depthwise Convolution : Analyse ind√©pendante des canaux EEG\n",
    "        self.conv3 = nn.Conv2d(32, 32, (1, 16), groups=32, padding=(0, 8))  \n",
    "        \n",
    "        # Pointwise Convolution : M√©lange des informations entre les canaux EEG\n",
    "        self.conv4 = nn.Conv2d(32, 32, (1, 1))  \n",
    "        self.batchnorm2 = nn.BatchNorm2d(32)  # Deuxi√®me normalisation\n",
    "        \n",
    "        # Couche Fully Connected : Transformation en repr√©sentation de haut niveau\n",
    "        self.fc1 = nn.Linear(32 * 2 * 50, 64)\n",
    "        self.dropout = nn.Dropout(0.5)  # R√©gularisation pour √©viter le sur-apprentissage\n",
    "        \n",
    "        # Couche de sortie : Pr√©diction d'une des 5 classes (stop, avant, arri√®re, gauche, droite)\n",
    "        self.fc_output = nn.Linear(64, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.elu(self.batchnorm1(self.conv1(x)))  # Activation apr√®s normalisation\n",
    "        x = F.elu(self.conv2(x))  \n",
    "        x = F.elu(self.batchnorm2(self.conv3(x)))  \n",
    "        x = F.elu(self.conv4(x))  \n",
    "        \n",
    "        x = x.view(x.size(0), -1)  # Flatten pour la couche fully connected\n",
    "        x = F.elu(self.fc1(x))\n",
    "        x = self.dropout(x)  # Appliquer Dropout pour r√©duire l'overfitting\n",
    "        \n",
    "        output = self.fc_output(x)  # Sortie finale du mod√®le\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S√©lection de l'appareil (GPU si disponible, sinon CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialisation du mod√®le et transfert sur l'appareil choisi\n",
    "model = EEGNet().to(device)\n",
    "\n",
    "# D√©finition de l'optimiseur (Adam) et de la fonction de perte (CrossEntropy)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Boucle d'entra√Ænement\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0  # Initialisation de la perte totale pour l'√©poque\n",
    "    \n",
    "    for eeg, label in dataloader:\n",
    "        # Transf√©rer les donn√©es sur l'appareil utilis√© (GPU/CPU)\n",
    "        eeg, label = eeg.to(device), label.to(device)\n",
    "        \n",
    "        # R√©initialisation des gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Passage avant (forward) : pr√©diction du mod√®le\n",
    "        output = model(eeg)\n",
    "        \n",
    "        # Calcul de la perte entre la pr√©diction et la v√©rit√© terrain\n",
    "        loss = criterion(output, label)\n",
    "        \n",
    "        # R√©tropropagation (backpropagation) pour calculer les gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # Mise √† jour des poids du mod√®le\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulation de la perte pour l'√©poque\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # Affichage de la perte moyenne par √©poque\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"eegnet_model.pth\")\n",
    "print(\"‚úÖ Mod√®le Entra√Æn√© et Sauvegard√© !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    all_true, all_pred = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for eeg, label in dataloader:\n",
    "            eeg, label = eeg.to(device), label.to(device)\n",
    "            output = model(eeg)\n",
    "            pred = output.argmax(dim=1)\n",
    "            all_true.extend(label.cpu().numpy())\n",
    "            all_pred.extend(pred.cpu().numpy())\n",
    "    \n",
    "    return all_true, all_pred\n",
    "\n",
    "def plot_confusion_matrix(true_labels, pred_labels, classes, title):\n",
    "    cm = confusion_matrix(true_labels, pred_labels)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "    plt.title(f\"Matrice de Confusion - {title}\")\n",
    "    plt.show()\n",
    "\n",
    "def show_classification_report(true_labels, pred_labels, classes, title):\n",
    "    print(f\"\\nüìä Rapport de Classification - {title}\")\n",
    "    print(classification_report(true_labels, pred_labels, target_names=classes))\n",
    "\n",
    "def analyze_errors(true_labels, pred_labels, classes, title):\n",
    "    errors = [(t, p) for t, p in zip(true_labels, pred_labels) if t != p]\n",
    "    unique_errors, counts = np.unique(errors, axis=0, return_counts=True)\n",
    "    print(f\"\\nüîç Pires Erreurs - {title}\")\n",
    "    for (true, pred), count in zip(unique_errors, counts):\n",
    "        print(f\"‚Üí Vrai: {classes[true]} | Pr√©dit: {classes[pred]} | Fois: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex√©cuter les analyses s√©par√©ment\n",
    "all_true, all_pred = evaluate_model(model, dataloader)\n",
    "classes = [\"stop\", \"avant\", \"arri√®re\", \"gauche\", \"droite\"]\n",
    "plot_confusion_matrix(all_true, all_pred, classes, \"Mouvements\")\n",
    "show_classification_report(all_true, all_pred, classes, \"Mouvements\")\n",
    "analyze_errors(all_true, all_pred, classes, \"Mouvements\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
